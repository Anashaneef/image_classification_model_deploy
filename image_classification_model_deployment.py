# -*- coding: utf-8 -*-
"""proyek_akhir_img_classification_model_deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aukKYFx1qkibTMmVPBEpDbXeeCp32lbX

Nama = Anas Fikri Hanif\
SIB ID = M183X0321

**Import Library**
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import zipfile, os
import shutil
import PIL
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from google.colab import files
from sklearn.model_selection import train_test_split

"""**Preparing Dataset from Kaggle**"""

# Install kaggle 
!pip install -q kaggle

# Token API
uploaded = files.upload()

# Receive dataset config
!chmod 600 /content/kaggle.json

# Download dataset
! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d prasunroy/natural-images

# extract dataset
local_zip = '/content/natural-images.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""**Data Information**"""

def list_files(path):
  files_num = 0
  for root, dirs, files in os.walk(path):
    level = root.replace(path, '').count(os.sep)
    indent = ' ' * 2 * (level)
    files_num += len(files)
    print('{}{}/ {}'.format(indent, os.path.basename(root), (str(len(files)) + ' images' if len(files) > 0 else '')))
  
  return files_num

base_dir = '/content/natural_images/'
list_files(base_dir)

"""**Delete Unused Data**"""

# Use flower, person, cat, fruit
!rm -rf '/content/natural_images/airplane'
!rm -rf '/content/natural_images/car'
!rm -rf '/content/natural_images/motorbike'
!rm -rf '/content/natural_images/dog'

"""**Check Image File Size**"""

# make a read_files function
def read_files(path):
  image_files = []
  for dirname, dirnames, filenames in os.walk(path):
    for filename in filenames:
      image_files.append(os.path.join(dirname, filename))
  
  return image_files

# check image size on entire folder
full_directory = read_files(base_dir)
image_sizes = []
for file in full_directory:
  image = PIL.Image.open(file)
  width, height = image.size
  image_sizes.append(f'{width}x{height}')

unique_sizes = set(image_sizes)

print(f'Image size list (first 15 unique size): \n{list(unique_sizes)[:15]}')

"""**Create Directory for Splitting Data**"""

train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'val')

cat_dir = os.path.join(base_dir, 'cat')
flower_dir = os.path.join(base_dir, 'flower')
fruit_dir = os.path.join(base_dir, 'fruit')
person_dir = os.path.join(base_dir, 'person')

# crate train and val directory
os.mkdir(train_dir)
os.mkdir(validation_dir)

# train
cat_train = os.path.join(train_dir, 'car')
flower_train = os.path.join(train_dir, 'flower')
fruit_train = os.path.join(train_dir, 'fruit')
person_train = os.path.join(train_dir, 'person')

# validation
cat_val = os.path.join(validation_dir, 'cat')
flower_val = os.path.join(validation_dir, 'flower')
fruit_val = os.path.join(validation_dir, 'fruit')
person_val = os.path.join(validation_dir, 'person')

# create train and val directory for each file
os.mkdir(cat_train)
os.mkdir(flower_train)
os.mkdir(fruit_train)
os.mkdir(person_train)
os.mkdir(cat_val)
os.mkdir(flower_val)
os.mkdir(fruit_val)
os.mkdir(person_val)

"""**Splitting Train and Test**"""

# test size 20%
cat_train_dir, cat_val_dir = train_test_split(os.listdir(cat_dir), test_size=0.20)
flower_train_dir, flower_val_dir = train_test_split(os.listdir(flower_dir), test_size=0.20)
fruit_train_dir, fruit_val_dir = train_test_split(os.listdir(fruit_dir), test_size=0.20)
person_train_dir, person_val_dir = train_test_split(os.listdir(person_dir), test_size=0.20)

# copy file for each train and val directory
for file in cat_train_dir:
  shutil.copy(os.path.join(cat_dir, file), os.path.join(cat_train, file))
for file in flower_train_dir:
  shutil.copy(os.path.join(flower_dir, file), os.path.join(flower_train, file))
for file in fruit_train_dir:
  shutil.copy(os.path.join(fruit_dir, file), os.path.join(fruit_train, file))
for file in person_train_dir:
  shutil.copy(os.path.join(person_dir, file), os.path.join(person_train, file))
for file in cat_val_dir:
  shutil.copy(os.path.join(cat_dir, file), os.path.join(cat_val, file))
for file in flower_val_dir:
  shutil.copy(os.path.join(flower_dir, file), os.path.join(flower_val, file))
for file in fruit_val_dir:
  shutil.copy(os.path.join(fruit_dir, file), os.path.join(fruit_val, file))
for file in person_val_dir:
  shutil.copy(os.path.join(person_dir, file), os.path.join(person_val, file))

"""**Augmentation & Image Generator**"""

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 20,
    horizontal_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest'
)

test_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 20,
    horizontal_flip = True,
    vertical_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest'
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size = (100,100),
    batch_size = 4,
    class_mode = 'categorical' 
)

validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size = (100,100),
    batch_size = 4,
    class_mode = 'categorical'
)

"""**Callback**"""

class myCallbacks(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>=0.93) and (logs.get('val_accuracy')>=0.93):
      print('\nAkurasi train dan test telah mencapai 93% !')
      self.model.stop_training = True
callbacks = myCallbacks()

"""**Create & Fit the Model**"""

# create the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(100, 100, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(100, 100, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(512, (3, 3), activation='relu', input_shape=(100, 100, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(4, activation='softmax')
])

# Show model summary
model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

# fit the model
epoch = 50
history = model.fit(
    train_generator,
    steps_per_epoch = 30,
    epochs = epoch,
    validation_data = validation_generator,
    validation_steps = 20,
    verbose = 2,
    callbacks = [callbacks]
)

"""**Training and Testing Accuracy & Loss Plot**"""

# plot accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

# plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

"""**TFLite Formatting**"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()

with tf.io.gfile.GFile('model_v1.tflite', 'wb') as f:
  f.write(tflite_model)